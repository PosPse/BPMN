{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "dataset_cora = Planetoid(root='./cora/', name='Cora')\n",
    "# dataset = Planetoid(root='./citeseer',name='Citeseer')\n",
    "# dataset = Planetoid(root='./pubmed/',name='Pubmed')\n",
    "print(dataset_cora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btr/miniconda3/envs/LLMEnG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
      "         [-1.8622, -0.9529, -2.4024,  ...,  1.0467,  1.3186,  1.0996],\n",
      "         [-0.2761, -0.4898, -0.9106,  ...,  1.2816,  0.6039,  1.7683],\n",
      "         ...,\n",
      "         [-0.3205, -2.7684, -2.1332,  ...,  1.1992,  0.9619,  1.0644],\n",
      "         [ 2.1264, -4.0235, -3.8833,  ..., -1.4924,  1.3204,  1.5962],\n",
      "         [-2.7754, -2.8158, -1.3051,  ..., -1.8278, -0.0244,  1.6202]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 定义模型和tokenizer的本地路径\n",
    "local_model_path = \"/home/btr/bpmn/model/safetensors/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# 加载预训练的tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModel.from_pretrained(local_model_path)\n",
    "\n",
    "# 要处理的输入文本\n",
    "text = \"这是一个示例文本\"\n",
    "\n",
    "# 将文本转换为tokenizer的输入格式\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型的最后一层隐藏状态作为嵌入表示\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 获取最后一层隐藏状态\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# 输出嵌入表示\n",
    "print(last_hidden_states)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 4096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset_cora.num_node_features, 16, 'lstm')\n",
    "        self.conv2 = SAGEConv(16, dataset_cora.num_classes, 'lstm')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(dataset_cora.num_node_features, 16, heads=2)\n",
    "        self.conv2 = GATConv(2*16, dataset_cora.num_classes, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 16, heads=2)\n",
      "  (conv2): GATConv(32, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GAT()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "data = dataset_cora[0].to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss: 1.9451 train_acc: 0.1857\n",
      "Epoch 001 train_loss: 1.9166 train_acc: 0.7214\n",
      "Epoch 002 train_loss: 1.8723 train_acc: 0.8071\n",
      "Epoch 003 train_loss: 1.8173 train_acc: 0.7786\n",
      "Epoch 004 train_loss: 1.7426 train_acc: 0.8000\n",
      "Epoch 005 train_loss: 1.6530 train_acc: 0.8929\n",
      "Epoch 006 train_loss: 1.5969 train_acc: 0.8714\n",
      "Epoch 007 train_loss: 1.5263 train_acc: 0.8571\n",
      "Epoch 008 train_loss: 1.4645 train_acc: 0.9143\n",
      "Epoch 009 train_loss: 1.4076 train_acc: 0.9500\n",
      "Epoch 010 train_loss: 1.3652 train_acc: 0.9429\n",
      "Epoch 011 train_loss: 1.3142 train_acc: 0.9714\n",
      "Epoch 012 train_loss: 1.2917 train_acc: 0.9571\n",
      "Epoch 013 train_loss: 1.2634 train_acc: 0.9714\n",
      "Epoch 014 train_loss: 1.2558 train_acc: 0.9714\n",
      "Epoch 015 train_loss: 1.2349 train_acc: 0.9857\n",
      "Epoch 016 train_loss: 1.2239 train_acc: 0.9786\n",
      "Epoch 017 train_loss: 1.2058 train_acc: 0.9857\n",
      "Epoch 018 train_loss: 1.2125 train_acc: 0.9714\n",
      "Epoch 019 train_loss: 1.2058 train_acc: 0.9786\n",
      "Epoch 020 train_loss: 1.1977 train_acc: 0.9786\n",
      "Epoch 021 train_loss: 1.1884 train_acc: 0.9929\n",
      "Epoch 022 train_loss: 1.1893 train_acc: 0.9857\n",
      "Epoch 023 train_loss: 1.1907 train_acc: 0.9857\n",
      "Epoch 024 train_loss: 1.1900 train_acc: 1.0000\n",
      "Epoch 025 train_loss: 1.1811 train_acc: 1.0000\n",
      "Epoch 026 train_loss: 1.1859 train_acc: 0.9857\n",
      "Epoch 027 train_loss: 1.1821 train_acc: 1.0000\n",
      "Epoch 028 train_loss: 1.1839 train_acc: 0.9929\n",
      "Epoch 029 train_loss: 1.1748 train_acc: 1.0000\n",
      "Epoch 030 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 031 train_loss: 1.1713 train_acc: 1.0000\n",
      "Epoch 032 train_loss: 1.1728 train_acc: 1.0000\n",
      "Epoch 033 train_loss: 1.1739 train_acc: 1.0000\n",
      "Epoch 034 train_loss: 1.1732 train_acc: 1.0000\n",
      "Epoch 035 train_loss: 1.1853 train_acc: 0.9929\n",
      "Epoch 036 train_loss: 1.1770 train_acc: 1.0000\n",
      "Epoch 037 train_loss: 1.1768 train_acc: 1.0000\n",
      "Epoch 038 train_loss: 1.1710 train_acc: 1.0000\n",
      "Epoch 039 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 040 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 041 train_loss: 1.1820 train_acc: 0.9929\n",
      "Epoch 042 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 043 train_loss: 1.1758 train_acc: 1.0000\n",
      "Epoch 044 train_loss: 1.1800 train_acc: 1.0000\n",
      "Epoch 045 train_loss: 1.1788 train_acc: 0.9929\n",
      "Epoch 046 train_loss: 1.1782 train_acc: 1.0000\n",
      "Epoch 047 train_loss: 1.1720 train_acc: 1.0000\n",
      "Epoch 048 train_loss: 1.1803 train_acc: 0.9929\n",
      "Epoch 049 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 050 train_loss: 1.1826 train_acc: 0.9929\n",
      "Epoch 051 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 052 train_loss: 1.1785 train_acc: 1.0000\n",
      "Epoch 053 train_loss: 1.1785 train_acc: 1.0000\n",
      "Epoch 054 train_loss: 1.1816 train_acc: 0.9929\n",
      "Epoch 055 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 056 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 057 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 058 train_loss: 1.1757 train_acc: 1.0000\n",
      "Epoch 059 train_loss: 1.1884 train_acc: 0.9857\n",
      "Epoch 060 train_loss: 1.1815 train_acc: 0.9929\n",
      "Epoch 061 train_loss: 1.1800 train_acc: 1.0000\n",
      "Epoch 062 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 063 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 064 train_loss: 1.1719 train_acc: 1.0000\n",
      "Epoch 065 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 066 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 067 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 068 train_loss: 1.1771 train_acc: 1.0000\n",
      "Epoch 069 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 070 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 071 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 072 train_loss: 1.1786 train_acc: 0.9929\n",
      "Epoch 073 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 074 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 075 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 076 train_loss: 1.1770 train_acc: 0.9929\n",
      "Epoch 077 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 078 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 079 train_loss: 1.1742 train_acc: 1.0000\n",
      "Epoch 080 train_loss: 1.1764 train_acc: 1.0000\n",
      "Epoch 081 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 082 train_loss: 1.1752 train_acc: 1.0000\n",
      "Epoch 083 train_loss: 1.1719 train_acc: 1.0000\n",
      "Epoch 084 train_loss: 1.1732 train_acc: 1.0000\n",
      "Epoch 085 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 086 train_loss: 1.1739 train_acc: 1.0000\n",
      "Epoch 087 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 088 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 089 train_loss: 1.1788 train_acc: 1.0000\n",
      "Epoch 090 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 091 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 092 train_loss: 1.1762 train_acc: 1.0000\n",
      "Epoch 093 train_loss: 1.1744 train_acc: 1.0000\n",
      "Epoch 094 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 095 train_loss: 1.1758 train_acc: 1.0000\n",
      "Epoch 096 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 097 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 098 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 099 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 100 train_loss: 1.1713 train_acc: 1.0000\n",
      "Epoch 101 train_loss: 1.1695 train_acc: 1.0000\n",
      "Epoch 102 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 103 train_loss: 1.1772 train_acc: 0.9929\n",
      "Epoch 104 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 105 train_loss: 1.1726 train_acc: 1.0000\n",
      "Epoch 106 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 107 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 108 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 109 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 110 train_loss: 1.1770 train_acc: 1.0000\n",
      "Epoch 111 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 112 train_loss: 1.1726 train_acc: 1.0000\n",
      "Epoch 113 train_loss: 1.1834 train_acc: 0.9857\n",
      "Epoch 114 train_loss: 1.1763 train_acc: 0.9929\n",
      "Epoch 115 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 116 train_loss: 1.1726 train_acc: 1.0000\n",
      "Epoch 117 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 118 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 119 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 120 train_loss: 1.1768 train_acc: 0.9929\n",
      "Epoch 121 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 122 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 123 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 124 train_loss: 1.1767 train_acc: 1.0000\n",
      "Epoch 125 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 126 train_loss: 1.1763 train_acc: 0.9929\n",
      "Epoch 127 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 128 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 129 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 130 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 131 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 132 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 133 train_loss: 1.1737 train_acc: 1.0000\n",
      "Epoch 134 train_loss: 1.1750 train_acc: 1.0000\n",
      "Epoch 135 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 136 train_loss: 1.1737 train_acc: 1.0000\n",
      "Epoch 137 train_loss: 1.1719 train_acc: 1.0000\n",
      "Epoch 138 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 139 train_loss: 1.1761 train_acc: 0.9929\n",
      "Epoch 140 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 141 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 142 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 143 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 144 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 145 train_loss: 1.1756 train_acc: 1.0000\n",
      "Epoch 146 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 147 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 148 train_loss: 1.1740 train_acc: 1.0000\n",
      "Epoch 149 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 150 train_loss: 1.1757 train_acc: 1.0000\n",
      "Epoch 151 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 152 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 153 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 154 train_loss: 1.1779 train_acc: 0.9929\n",
      "Epoch 155 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 156 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 157 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 158 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 159 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 160 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 161 train_loss: 1.1751 train_acc: 0.9929\n",
      "Epoch 162 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 163 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 164 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 165 train_loss: 1.1778 train_acc: 0.9929\n",
      "Epoch 166 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 167 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 168 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 169 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 170 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 171 train_loss: 1.1780 train_acc: 1.0000\n",
      "Epoch 172 train_loss: 1.1750 train_acc: 1.0000\n",
      "Epoch 173 train_loss: 1.1799 train_acc: 0.9929\n",
      "Epoch 174 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 175 train_loss: 1.1739 train_acc: 1.0000\n",
      "Epoch 176 train_loss: 1.1768 train_acc: 0.9929\n",
      "Epoch 177 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 178 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 179 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 180 train_loss: 1.1761 train_acc: 0.9929\n",
      "Epoch 181 train_loss: 1.1712 train_acc: 1.0000\n",
      "Epoch 182 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 183 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 184 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 185 train_loss: 1.1801 train_acc: 0.9929\n",
      "Epoch 186 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 187 train_loss: 1.1747 train_acc: 0.9929\n",
      "Epoch 188 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 189 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 190 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 191 train_loss: 1.1743 train_acc: 1.0000\n",
      "Epoch 192 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 193 train_loss: 1.1767 train_acc: 0.9929\n",
      "Epoch 194 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 195 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 196 train_loss: 1.1714 train_acc: 1.0000\n",
      "Epoch 197 train_loss: 1.1734 train_acc: 1.0000\n",
      "Epoch 198 train_loss: 1.1694 train_acc: 1.0000\n",
      "Epoch 199 train_loss: 1.1711 train_acc: 1.0000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, pred = torch.max(out[data.train_mask], dim=1)\n",
    "    correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "    acc = correct/data.train_mask.sum().item()\n",
    "\n",
    "    print('Epoch {:03d} train_loss: {:.4f} train_acc: {:.4f}'.format(\n",
    "        epoch, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b68f77c973245ceab472246e98a6ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 128256\n",
      "After we add 2 tokens\n",
      "vocabulary size: 128258\n",
      "torch.Size([128258, 4096])\n",
      "tensor([[-0.0186, -0.0514,  0.0089,  ...,  0.0045,  0.0186, -0.0358],\n",
      "        [ 0.0263, -0.0249,  0.0034,  ..., -0.0169,  0.0155, -0.0097]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"/home/btr/bpmn/model/safetensors/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "num_added_toks = tokenizer.add_tokens(['[ENT_START]', '[ENT_END]'], special_tokens=True)\n",
    "print(\"After we add\", num_added_toks, \"tokens\")\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.embed_tokens.weight.size())\n",
    "\n",
    "# Randomly generated matrix\n",
    "print(model.embed_tokens.weight[-2:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.embed_tokens.weight[-2:, :] = torch.zeros([2, model.config.hidden_size], requires_grad=True)\n",
    "print(model.embed_tokens.weight[-2:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end', 'of', 'entity']\n",
      "['start', 'of', 'entity']\n",
      "tensor([[-0.0340, -0.0144, -0.0441,  ..., -0.0016,  0.0318, -0.0151],\n",
      "        [-0.0060, -0.0202, -0.0312,  ..., -0.0084,  0.0193, -0.0296]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "descriptions = ['start of entity', 'end of entity']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, token in enumerate(reversed(descriptions), start=1):\n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        print(tokenized)\n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        new_embedding = model.embeddings.word_embeddings.weight[tokenized_ids].mean(axis=0)\n",
    "        model.embeddings.word_embeddings.weight[-i, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6809, -0.2879, -0.8822,  ..., -0.0319,  0.6351, -0.3029],\n",
      "        [-0.1190, -0.4035, -0.6236,  ..., -0.1687,  0.3868, -0.5921]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings.word_embeddings.weight[-2:, :] * 10 + model.embeddings.word_embeddings.weight[-2:, :]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重向量: tensor([0.5544, 0.4456], grad_fn=<SoftmaxBackward0>)\n",
      "扩展后的权重向量: tensor([[0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544],\n",
      "        [0.5544, 0.5544, 0.5544]], grad_fn=<ExpandBackward0>)\n",
      "加权求和结果: tensor([[ 2.1220, -0.7132,  0.3025],\n",
      "        [-0.2825,  0.2893,  0.3864],\n",
      "        [ 0.0700,  1.7125, -1.0699],\n",
      "        [ 0.7106,  0.9815, -0.2224],\n",
      "        [-0.1663, -0.2572,  0.0807],\n",
      "        [ 0.4351,  1.0679,  0.3167],\n",
      "        [-0.1876,  0.3540,  0.6517],\n",
      "        [-0.6215, -0.5523,  0.1861],\n",
      "        [ 0.0147,  1.1826,  0.8368],\n",
      "        [ 1.2769,  0.4824,  0.1356]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设我们有两个特征张量，分别是文本嵌入和节点类型嵌入\n",
    "# 假设它们的形状是 [batch_size, feature_size]\n",
    "text_embedding = torch.randn(10, 3)  # 随机生成示例数据\n",
    "node_type_embedding = torch.randn(10, 3)  # 同上\n",
    "\n",
    "# 定义权重向量，权重可以根据需要进行调整\n",
    "# 假设权重是可学习的参数，这里我们随机初始化\n",
    "weights = torch.nn.Parameter(torch.randn(2))\n",
    "\n",
    "# 计算权重的指数，使得权重的和为1（softmax）\n",
    "weights = torch.softmax(weights, dim=0)\n",
    "print(\"权重向量:\", weights)\n",
    "# 将权重扩展到特征张量的形状\n",
    "weights_text = weights[0].expand_as(text_embedding)\n",
    "print(\"扩展后的权重向量:\", weights_text)\n",
    "weights_node_type = weights[1].expand_as(node_type_embedding)\n",
    "\n",
    "# 计算加权求和\n",
    "weighted_sum = (text_embedding * weights_text) + (node_type_embedding * weights_node_type)\n",
    "\n",
    "print(\"加权求和结果:\", weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.3854\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m gcn_classifier(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMEnG/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMEnG/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMEnG/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, node_type_dim, output_dim):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        # 假设文本嵌入和节点类型嵌入的维度分别是text_embedding_dim和node_type_dim\n",
    "        self.fc_text = nn.Linear(text_embedding_dim, output_dim)\n",
    "        self.fc_node_type = nn.Linear(node_type_dim, output_dim)\n",
    "        self.fc_fusion = nn.Linear(output_dim, output_dim)\n",
    "        \n",
    "        # 初始化权重参数\n",
    "        self.weights = nn.Parameter(torch.randn(2))\n",
    "\n",
    "    def forward(self, text_embedding, node_type_embedding):\n",
    "        # 分别对文本嵌入和节点类型嵌入应用全连接层\n",
    "        text_output = self.fc_text(text_embedding)\n",
    "        node_type_output = self.fc_node_type(node_type_embedding)\n",
    "        \n",
    "        # 使用权重对输出进行加权\n",
    "        text_weighted = text_output * self.weights[0]\n",
    "        node_type_weighted = node_type_output * self.weights[1]\n",
    "        \n",
    "        # 将加权结果合并并再次通过全连接层\n",
    "        fused_output = self.fc_fusion(text_weighted + node_type_weighted)\n",
    "        return fused_output\n",
    "\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 第一层图卷积\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        # 第二层图卷积\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 假设我们有输入数据和目标\n",
    "# 这里我们使用随机数据作为示例\n",
    "num_nodes = 1000\n",
    "num_edges = 2000\n",
    "edge_index = torch.randint(0, num_nodes, (2, num_edges))\n",
    "text_embedding = torch.randn((num_nodes, 16))  # 假设每个节点有16维的文本嵌入\n",
    "node_type_embedding = torch.randn((num_nodes, 8))  # 假设每个节点有8维的节点类型嵌入\n",
    "y = torch.randint(0, 4, (num_nodes,))  # 假设有4个类别\n",
    "\n",
    "# 实例化特征融合模块和GCN分类器\n",
    "feature_fusion = FeatureFusionModule(16, 8, 32)  # 假设输出融合特征维度为32\n",
    "gcn_classifier = GCNClassifier(32, 64, 4)  # 假设GCN的隐藏层维度为64，输出类别数为4\n",
    "\n",
    "# 将特征融合模块的输出作为GCN分类器的输入\n",
    "combined_features = feature_fusion(text_embedding, node_type_embedding)\n",
    "\n",
    "# 将图结构信息和特征信息整合到Data对象中\n",
    "data = Data(x=combined_features, edge_index=edge_index, y=y)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(feature_fusion.parameters()) + list(gcn_classifier.parameters()), lr=0.01)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = gcn_classifier(data.x, data.edge_index)\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 训练完成后，feature_fusion.weights 和 gcn_classifier 的参数都学习到了\n",
    "print(\"学习到的特征融合权重:\", feature_fusion.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3948e-01, -1.2277e-01, -1.0967e-01,  ..., -2.0057e-01,\n",
      "           3.4953e-01,  7.2549e-01],\n",
      "         [ 3.9157e-01,  1.0379e-01, -6.5234e-01,  ..., -5.1784e-04,\n",
      "           7.4655e-01,  4.0899e-01],\n",
      "         [ 7.3401e-01,  3.4777e-01, -3.3854e-03,  ..., -2.0010e-02,\n",
      "          -5.1007e-01, -4.2176e-01],\n",
      "         ...,\n",
      "         [ 6.5899e-01, -3.5357e-01, -2.2464e-01,  ..., -5.1779e-01,\n",
      "          -3.7647e-01,  5.5006e-01],\n",
      "         [ 6.7845e-01,  2.9403e-01, -2.0974e-01,  ...,  2.5586e-01,\n",
      "          -2.7047e-01, -5.8366e-01],\n",
      "         [-1.6920e-02,  1.0900e-01,  1.5850e-01,  ...,  6.9561e-01,\n",
      "          -1.6555e-01, -9.8933e-02]],\n",
      "\n",
      "        [[ 1.5857e-01,  4.8041e-01, -1.7775e-01,  ..., -2.5178e-01,\n",
      "           3.5022e-01,  3.7165e-01],\n",
      "         [ 2.8165e-01,  4.2239e-01, -3.7917e-01,  ..., -1.3355e-02,\n",
      "           8.8876e-01,  4.2816e-01],\n",
      "         [ 4.6219e-01,  6.3719e-01,  3.8362e-01,  ...,  2.5919e-01,\n",
      "           1.8919e-01,  3.1840e-01],\n",
      "         ...,\n",
      "         [ 3.0722e-01,  5.6520e-01,  4.6095e-01,  ...,  1.5050e-01,\n",
      "           1.5375e-02,  1.4220e-01],\n",
      "         [ 2.7022e-01,  4.5986e-01,  4.8362e-01,  ...,  1.6021e-01,\n",
      "          -3.2939e-02,  2.1034e-01],\n",
      "         [ 2.1594e-01,  4.0219e-01,  4.2181e-01,  ...,  1.3091e-01,\n",
      "          -7.9845e-02,  2.5772e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.1395, -0.1228, -0.1097,  ..., -0.2006,  0.3495,  0.7255],\n",
      "        [ 0.1586,  0.4804, -0.1778,  ..., -0.2518,  0.3502,  0.3716]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([2, 768])\n",
      "tensor([[-0.7919, -0.3541, -0.5460,  ..., -0.3982, -0.5416,  0.8997],\n",
      "        [-0.9194, -0.4750, -0.8049,  ..., -0.3834, -0.7398,  0.9459]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 定义模型和tokenizer的本地路径\n",
    "local_model_path = \"/home/btr/bpmn/model/safetensors/bert-base-uncased\"\n",
    "\n",
    "# 加载预训练的tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModel.from_pretrained(local_model_path)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state)  # 输出形状 (batch_size, sequence_length, hidden_size)\n",
    "print(outputs.last_hidden_state[:, 0, :])  # 输出形状 (batch_size, hidden_sizequence_length, hidden_size)\n",
    "print(outputs.last_hidden_state[:, 0, :].shape)  # 输出形状 (batch_size, hidden_size)\n",
    "print(outputs.pooler_output)  # 输出形状 (batch_size, hidden_size)\n",
    "# # 获取嵌入表示\n",
    "# embeddings = []\n",
    "# for text in texts:\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     # 取最后一层的[CLS] token作为文本的表示\n",
    "#     cls_embedding = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "#     embeddings.append(cls_embedding)\n",
    "# print(embeddings[0].shape)  # (1, hidden_size)\n",
    "# # 整理为一个Tensor\n",
    "# text_embeddings = torch.cat(embeddings, dim=0)\n",
    "# print(text_embeddings.shape)  # (num_texts, hidden_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625dfa8a05e440198044b1ea5ed4c8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 128256\n",
      "After we add 6 tokens\n",
      "vocabulary size: 128262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [sign-loop] cyclic signal words\n",
      "2 [sign-parallel] parallel signal words\n",
      "3 [sign-selection] selective signal words\n",
      "4 [sign-successor] sequential signal words\n",
      "5 [condition] gateway conditions\n",
      "6 [activity] activity event entity\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaModel\n",
    "import torch\n",
    "checkpoint = \"/home/btr/bpmn/model/safetensors/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlamaModel.from_pretrained(checkpoint)\n",
    "\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "num_added_toks = tokenizer.add_tokens(['[activity]', '[condition]', '[sign-successor]', '[sign-selection]', '[sign-parallel]', '[sign-loop]'], special_tokens=True)\n",
    "print(\"After we add\", num_added_toks, \"tokens\")\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "new_token = {'[activity]': \"activity event entity\",\n",
    "                     '[condition]': \"gateway conditions\",\n",
    "                     '[sign-successor]': \"sequential signal words\",\n",
    "                     '[sign-selection]': \"selective signal words\",\n",
    "                     '[sign-parallel]': \"parallel signal words\",\n",
    "                     '[sign-loop]': \"cyclic signal words\"}\n",
    "with torch.no_grad():\n",
    "    for i, (k, v) in enumerate(reversed(new_token.items()), start=1):\n",
    "        print(i, k, v)\n",
    "        tokenized = tokenizer.tokenize(v)\n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        new_token_emb = model.embed_tokens.weight[tokenized_ids].mean(dim=0)\n",
    "        model.embed_tokens.weight[-i, :] = new_token_emb.clone().detach().requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(['[activity]', '[condition]'], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb9153dad0545b3b2d02d988b304dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaModel, BertModel\n",
    "import torch\n",
    "checkpoint = \"/home/btr/bpmn/model/safetensors/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = LlamaModel.from_pretrained(checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128256, 4096])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaModel' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241m.\u001b[39mwte\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLMEnG/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaModel' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "print(model.embed_tokens.weight.size())\n",
    "print(model.transformer.wte.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体均值: tensor(3.5000)\n",
      "沿着行的均值: tensor([2.5000, 3.5000, 4.5000])\n",
      "沿着列的均值: tensor([2., 5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个二维张量\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "# 计算整个张量的均值\n",
    "mean_all = torch.mean(tensor_2d)\n",
    "\n",
    "# 沿着特定维度计算均值\n",
    "mean_dim0 = torch.mean(tensor_2d, dim=0)  # 沿着行计算均值\n",
    "mean_dim1 = torch.mean(tensor_2d, dim=1)  # 沿着列计算均值\n",
    "\n",
    "print(\"整体均值:\", mean_all)\n",
    "print(\"沿着行的均值:\", mean_dim0)\n",
    "print(\"沿着列的均值:\", mean_dim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "checkpoint = \"/home/btr/bpmn/model/safetensors/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "print(model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "稀疏矩阵：\n",
      "  (0, 1)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t-1\n",
      "  (2, 1)\t1\n",
      "转换回的邻接矩阵：\n",
      "[[ 0  2  0]\n",
      " [ 1  0 -1]\n",
      " [ 0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "\n",
    "# 创建一个邻接矩阵\n",
    "adj_matrix = torch.tensor([[0, 2, 0],\n",
    "                        [1, 0, -1],\n",
    "                        [0, 1, 0]])\n",
    "\n",
    "# 邻接矩阵转稀疏矩阵\n",
    "sparse_matrix = csr_matrix(adj_matrix)\n",
    "print(\"稀疏矩阵：\")\n",
    "print(sparse_matrix)\n",
    "\n",
    "# 稀疏矩阵转邻接矩阵\n",
    "adj_matrix_converted = sparse_matrix.toarray()\n",
    "print(\"转换回的邻接矩阵：\")\n",
    "print(adj_matrix_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "邻接表: {0: [1, 2], 1: [0, 3], 2: [0, 3], 3: [1, 2]}\n",
      "邻接矩阵: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "def adj_matrix_to_adj_list(matrix):\n",
    "    adj_list = {}\n",
    "    n = len(matrix)\n",
    "    for i in range(n):\n",
    "        adj_list[i] = [j for j in range(n) if matrix[i][j] != 0]\n",
    "    return adj_list\n",
    "\n",
    "def adj_list_to_adj_matrix(adj_list):\n",
    "    n = len(adj_list)\n",
    "    matrix = [[0] * n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in adj_list[i]:\n",
    "            matrix[i][j] = 1  # 可以根据需求修改为权重\n",
    "    return matrix\n",
    "# 示例邻接矩阵\n",
    "adj_matrix = [\n",
    "    [0, 2, 3, 0],\n",
    "    [1, 0, 9, 1],\n",
    "    [1, 0, 0, 1],\n",
    "    [0, 1, 1, 0]\n",
    "]\n",
    "\n",
    "# 转换为邻接表\n",
    "adj_list = adj_matrix_to_adj_list(adj_matrix)\n",
    "print(\"邻接表:\", adj_list)\n",
    "\n",
    "# 从邻接表转换回邻接矩阵\n",
    "new_adj_matrix = adj_list_to_adj_matrix(adj_list)\n",
    "print(\"邻接矩阵:\", new_adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 1, 1, 2, 0, 3],\n",
      "        [1, 0, 0, 0, 0, 1, 3, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "x = torch.tensor([[2,1], [5,6], [3,7], [12,0]])\n",
    "y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "edge_index = torch.tensor([[0, 1, 1, 1, 1, 2, 0, 3], \n",
    "                           [1, 0, 0, 0, 0, 1, 3, 2]])\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = [1,2,3,4,0,0,0,1,2,0]\n",
    "b = [1,3,2,4,0,0,0,1,3,1]\n",
    "c = filter(lambda x: x != 0, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.9296, 0.7615, 0.3728,  ..., 0.1704, 0.8016, 0.0198],\n",
      "         [0.6924, 0.6742, 0.3752,  ..., 0.5237, 0.5476, 0.9446],\n",
      "         [0.6509, 0.8919, 0.0971,  ..., 0.7914, 0.8428, 0.6248],\n",
      "         ...,\n",
      "         [0.9952, 0.5852, 0.2957,  ..., 0.1280, 0.8672, 0.7934],\n",
      "         [0.7923, 0.5912, 0.5387,  ..., 0.4217, 0.8553, 0.2172],\n",
      "         [0.8531, 0.3162, 0.3026,  ..., 0.5167, 0.3850, 0.0667]]])\n",
      "torch.Size([3, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(20, 128)\n",
    "b = torch.zeros(20, 128)\n",
    "c = torch.rand(20, 128)\n",
    "\n",
    "# 将a, b, c组合成（3, 20, 128）\n",
    "combined = torch.stack([a, b, c], dim=0)\n",
    "print(combined)  # 输出: torch.Size([3, 20, 128])\n",
    "print(combined.shape)  # 输出: torch.Size([3, 20, 128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4010,  1.1473,  1.0520,  1.0520,  0.9907,  0.9588,  0.9324, -0.4404,\n",
      "        -0.4440, -0.8637, -0.8830, -0.8994, -0.9586, -0.9762, -1.0267, -1.0420]) tensor(7.4506e-08) tensor(1.)\n",
      "tensor([1.0000, 0.8962, 0.8571, 0.8571, 0.8321, 0.8190, 0.8082, 0.2463, 0.2448,\n",
      "        0.0730, 0.0651, 0.0584, 0.0341, 0.0269, 0.0063, 0.0000]) tensor(0.4265) tensor(0.4093)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "# Set print options# Define a tensor\n",
    "a = torch.tensor(\n",
    "    [-1.8297, -1.9489, -1.9937, -1.9937, -2.0225, -2.0375, -2.0499,\n",
    "     -2.6950, -2.6967, -2.8939, -2.9030, -2.9107, -2.9385, -2.9468,\n",
    "     -2.9705, -2.9777])\n",
    "# a = torch.sort(input=a, descending=False).values\n",
    "# print(a, torch.mean(a), torch.std(a))\n",
    " \n",
    "# Z-score standardization\n",
    "mean_a = torch.mean(a)\n",
    "std_a = torch.std(a)\n",
    "n1 = (a - mean_a) / std_a\n",
    "print(n1, torch.mean(n1), torch.std(n1))\n",
    " \n",
    "# Min-Max scaling\n",
    "min_a = torch.min(a)\n",
    "max_a = torch.max(a)\n",
    "n2 = (a - min_a) / (max_a - min_a)\n",
    "print(n2, torch.mean(n2), torch.std(n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原矩阵：\n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]\n",
      " [30 31 32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47 48 49]\n",
      " [50 51 52 53 54 55 56 57 58 59]\n",
      " [60 61 62 63 64 65 66 67 68 69]\n",
      " [70 71 72 73 74 75 76 77 78 79]\n",
      " [80 81 82 83 84 85 86 87 88 89]\n",
      " [90 91 92 93 94 95 96 97 98 99]]\n",
      "\n",
      "提取的4x4子矩阵：\n",
      "子矩阵 1:\n",
      "[[ 0  1  2]\n",
      " [10 11 12]\n",
      " [20 21 22]]\n",
      "子矩阵 2:\n",
      "[[11 12 13]\n",
      " [21 22 23]\n",
      " [31 32 33]]\n",
      "子矩阵 3:\n",
      "[[22 23 24]\n",
      " [32 33 34]\n",
      " [42 43 44]]\n",
      "子矩阵 4:\n",
      "[[33 34 35]\n",
      " [43 44 45]\n",
      " [53 54 55]]\n",
      "子矩阵 5:\n",
      "[[44 45 46]\n",
      " [54 55 56]\n",
      " [64 65 66]]\n",
      "子矩阵 6:\n",
      "[[55 56 57]\n",
      " [65 66 67]\n",
      " [75 76 77]]\n",
      "子矩阵 7:\n",
      "[[66 67 68]\n",
      " [76 77 78]\n",
      " [86 87 88]]\n",
      "子矩阵 8:\n",
      "[[77 78 79]\n",
      " [87 88 89]\n",
      " [97 98 99]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个10x10的矩阵\n",
    "matrix = np.arange(100).reshape(10, 10)\n",
    "\n",
    "# 输出原矩阵\n",
    "print(\"原矩阵：\")\n",
    "print(matrix)\n",
    "\n",
    "# 提取4x4子矩阵\n",
    "sub_matrices = []\n",
    "\n",
    "for i in range(8):  # 由于是4x4的框，行和列的最大起始索引是7\n",
    "    sub_matrix = matrix[i:i+3, i:i+3]\n",
    "    sub_matrices.append(sub_matrix)\n",
    "\n",
    "# 输出提取的4x4子矩阵\n",
    "print(\"\\n提取的4x4子矩阵：\")\n",
    "for idx, sub_matrix in enumerate(sub_matrices):\n",
    "    print(f\"子矩阵 {idx+1}:\")\n",
    "    print(sub_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph MyPicture {\n",
      "\ta [label=Ming color=green]\n",
      "\tb [label=Hong color=yellow]\n",
      "\tc [label=Dong]\n",
      "\ta -> b [label=\"ab\n",
      "a-b\" color=red]\n",
      "\tc -> b\n",
      "\ta -> c\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/btr/bpmn/LLMEnG/MyPicture.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# 实例化一个Digraph对象(有向图)，name:生成的图片的图片名，format:生成的图片格式\n",
    "dot = Digraph(name=\"MyPicture\", format=\"png\")\n",
    "\n",
    "# 生成图片节点，name：这个节点对象的名称，label:节点名,color：画节点的线的颜色\n",
    "dot.node(name='a', label='Ming', color='green')\n",
    "dot.node(name='b', label='Hong', color='yellow')\n",
    "dot.node(name='c', label='Dong')\n",
    "\n",
    "# 在节点之间画线，label：线上显示的文本,color:线的颜色\n",
    "dot.edge('a', 'b', label=\"ab\\na-b\", color='red')\n",
    "# 一次性画多条线，c到b的线，a到c的线\n",
    "dot.edges(['cb', 'ac'])\n",
    "\n",
    "# 打印生成的源代码\n",
    "print(dot.source)\n",
    "\n",
    "\n",
    "# 跟view一样的用法(render跟view选择一个即可)，一般用render生成图片，不使用view=True,view=True用在调试的时候\n",
    "dot.render(filename='MyPicture', directory=\"/home/btr/bpmn/LLMEnG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello.gv.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" rule for type \"application/pdf\" passed its test case\n",
      "       (for more information, add \"--debug=1\" on the command line)\n",
      "Loading \"original-fs\" failed\n",
      "Error: Cannot find module 'original-fs'\n",
      "Require stack:\n",
      "- /home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js\n",
      "\u001b[90m    at Module._resolveFilename (node:internal/modules/cjs/loader:1145:15)\u001b[39m\n",
      "\u001b[90m    at Module._load (node:internal/modules/cjs/loader:986:27)\u001b[39m\n",
      "\u001b[90m    at Module.require (node:internal/modules/cjs/loader:1233:19)\u001b[39m\n",
      "\u001b[90m    at require (node:internal/modules/helpers:179:18)\u001b[39m\n",
      "    at i (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:98)\n",
      "    at r.load (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:2:1637)\n",
      "    at h.load (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:13958)\n",
      "    at u (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9338)\n",
      "    at Object.errorback (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9457)\n",
      "    at h.triggerErrorback (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:14252)\n",
      "    at /home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:14003\n",
      "    at r.load (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:2:1654)\n",
      "    at h.load (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:13958)\n",
      "    at u (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9338)\n",
      "    at l._loadModule (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9466)\n",
      "    at l._resolve (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:452)\n",
      "    at l.defineModule (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:5561)\n",
      "    at Function.p [as define] (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:1741)\n",
      "    at out-build/bootstrap-amd.js (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:6445)\n",
      "    at /home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:132\n",
      "    at Object.<anonymous> (/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:9653)\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1358:14)\u001b[39m\n",
      "\u001b[90m    at Module._extensions..js (node:internal/modules/cjs/loader:1416:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1208:32)\u001b[39m\n",
      "\u001b[90m    at Module._load (node:internal/modules/cjs/loader:1024:12)\u001b[39m\n",
      "\u001b[90m    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)\u001b[39m\n",
      "\u001b[90m    at node:internal/main/run_main_module:28:49\u001b[39m {\n",
      "  code: \u001b[32m'MODULE_NOT_FOUND'\u001b[39m,\n",
      "  requireStack: [\n",
      "    \u001b[32m'/home/btr/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js'\u001b[39m\n",
      "  ],\n",
      "  phase: \u001b[32m'loading'\u001b[39m,\n",
      "  moduleId: \u001b[32m'original-fs'\u001b[39m,\n",
      "  neededBy: [ \u001b[32m'fs'\u001b[39m ]\n",
      "}\n",
      "Here are the modules that depend on it:\n",
      "[ \u001b[32m'fs'\u001b[39m ]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
