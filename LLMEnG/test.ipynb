{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "dataset_cora = Planetoid(root='./cora/', name='Cora')\n",
    "# dataset = Planetoid(root='./citeseer',name='Citeseer')\n",
    "# dataset = Planetoid(root='./pubmed/',name='Pubmed')\n",
    "print(dataset_cora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset_cora.num_node_features, 16, 'lstm')\n",
    "        self.conv2 = SAGEConv(16, dataset_cora.num_classes, 'lstm')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(dataset_cora.num_node_features, 16, heads=2)\n",
    "        self.conv2 = GATConv(2*16, dataset_cora.num_classes, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 16, heads=2)\n",
      "  (conv2): GATConv(32, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GAT()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "data = dataset_cora[0].to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss: 1.1844 train_acc: 0.9857\n",
      "Epoch 001 train_loss: 1.1903 train_acc: 0.9857\n",
      "Epoch 002 train_loss: 1.1836 train_acc: 0.9929\n",
      "Epoch 003 train_loss: 1.1756 train_acc: 0.9929\n",
      "Epoch 004 train_loss: 1.1865 train_acc: 0.9857\n",
      "Epoch 005 train_loss: 1.1806 train_acc: 0.9929\n",
      "Epoch 006 train_loss: 1.1847 train_acc: 0.9857\n",
      "Epoch 007 train_loss: 1.1887 train_acc: 0.9857\n",
      "Epoch 008 train_loss: 1.1868 train_acc: 0.9857\n",
      "Epoch 009 train_loss: 1.1761 train_acc: 0.9929\n",
      "Epoch 010 train_loss: 1.1771 train_acc: 1.0000\n",
      "Epoch 011 train_loss: 1.1847 train_acc: 0.9857\n",
      "Epoch 012 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 013 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 014 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 015 train_loss: 1.1743 train_acc: 1.0000\n",
      "Epoch 016 train_loss: 1.1744 train_acc: 1.0000\n",
      "Epoch 017 train_loss: 1.1793 train_acc: 0.9929\n",
      "Epoch 018 train_loss: 1.1695 train_acc: 1.0000\n",
      "Epoch 019 train_loss: 1.1751 train_acc: 1.0000\n",
      "Epoch 020 train_loss: 1.1778 train_acc: 1.0000\n",
      "Epoch 021 train_loss: 1.1779 train_acc: 0.9929\n",
      "Epoch 022 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 023 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 024 train_loss: 1.1763 train_acc: 1.0000\n",
      "Epoch 025 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 026 train_loss: 1.1780 train_acc: 0.9929\n",
      "Epoch 027 train_loss: 1.1699 train_acc: 1.0000\n",
      "Epoch 028 train_loss: 1.1748 train_acc: 1.0000\n",
      "Epoch 029 train_loss: 1.1702 train_acc: 1.0000\n",
      "Epoch 030 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 031 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 032 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 033 train_loss: 1.1696 train_acc: 1.0000\n",
      "Epoch 034 train_loss: 1.1684 train_acc: 1.0000\n",
      "Epoch 035 train_loss: 1.1714 train_acc: 1.0000\n",
      "Epoch 036 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 037 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 038 train_loss: 1.1694 train_acc: 1.0000\n",
      "Epoch 039 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 040 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 041 train_loss: 1.1732 train_acc: 1.0000\n",
      "Epoch 042 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 043 train_loss: 1.1696 train_acc: 1.0000\n",
      "Epoch 044 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 045 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 046 train_loss: 1.1768 train_acc: 1.0000\n",
      "Epoch 047 train_loss: 1.1737 train_acc: 0.9929\n",
      "Epoch 048 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 049 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 050 train_loss: 1.1685 train_acc: 1.0000\n",
      "Epoch 051 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 052 train_loss: 1.1687 train_acc: 1.0000\n",
      "Epoch 053 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 054 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 055 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 056 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 057 train_loss: 1.1693 train_acc: 1.0000\n",
      "Epoch 058 train_loss: 1.1687 train_acc: 1.0000\n",
      "Epoch 059 train_loss: 1.1688 train_acc: 1.0000\n",
      "Epoch 060 train_loss: 1.1710 train_acc: 1.0000\n",
      "Epoch 061 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 062 train_loss: 1.1691 train_acc: 1.0000\n",
      "Epoch 063 train_loss: 1.1691 train_acc: 1.0000\n",
      "Epoch 064 train_loss: 1.1705 train_acc: 1.0000\n",
      "Epoch 065 train_loss: 1.1712 train_acc: 1.0000\n",
      "Epoch 066 train_loss: 1.1699 train_acc: 1.0000\n",
      "Epoch 067 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 068 train_loss: 1.1774 train_acc: 0.9929\n",
      "Epoch 069 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 070 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 071 train_loss: 1.1689 train_acc: 1.0000\n",
      "Epoch 072 train_loss: 1.1753 train_acc: 1.0000\n",
      "Epoch 073 train_loss: 1.1682 train_acc: 1.0000\n",
      "Epoch 074 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 075 train_loss: 1.1702 train_acc: 1.0000\n",
      "Epoch 076 train_loss: 1.1685 train_acc: 1.0000\n",
      "Epoch 077 train_loss: 1.1692 train_acc: 1.0000\n",
      "Epoch 078 train_loss: 1.1786 train_acc: 0.9929\n",
      "Epoch 079 train_loss: 1.1689 train_acc: 1.0000\n",
      "Epoch 080 train_loss: 1.1697 train_acc: 1.0000\n",
      "Epoch 081 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 082 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 083 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 084 train_loss: 1.1737 train_acc: 0.9929\n",
      "Epoch 085 train_loss: 1.1693 train_acc: 1.0000\n",
      "Epoch 086 train_loss: 1.1679 train_acc: 1.0000\n",
      "Epoch 087 train_loss: 1.1746 train_acc: 1.0000\n",
      "Epoch 088 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 089 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 090 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 091 train_loss: 1.1691 train_acc: 1.0000\n",
      "Epoch 092 train_loss: 1.1751 train_acc: 0.9929\n",
      "Epoch 093 train_loss: 1.1694 train_acc: 1.0000\n",
      "Epoch 094 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 095 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 096 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 097 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 098 train_loss: 1.1712 train_acc: 1.0000\n",
      "Epoch 099 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 100 train_loss: 1.1744 train_acc: 1.0000\n",
      "Epoch 101 train_loss: 1.1705 train_acc: 1.0000\n",
      "Epoch 102 train_loss: 1.1694 train_acc: 1.0000\n",
      "Epoch 103 train_loss: 1.1714 train_acc: 1.0000\n",
      "Epoch 104 train_loss: 1.1689 train_acc: 1.0000\n",
      "Epoch 105 train_loss: 1.1697 train_acc: 1.0000\n",
      "Epoch 106 train_loss: 1.1694 train_acc: 1.0000\n",
      "Epoch 107 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 108 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 109 train_loss: 1.1690 train_acc: 1.0000\n",
      "Epoch 110 train_loss: 1.1705 train_acc: 1.0000\n",
      "Epoch 111 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 112 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 113 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 114 train_loss: 1.1705 train_acc: 1.0000\n",
      "Epoch 115 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 116 train_loss: 1.1696 train_acc: 1.0000\n",
      "Epoch 117 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 118 train_loss: 1.1710 train_acc: 1.0000\n",
      "Epoch 119 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 120 train_loss: 1.1702 train_acc: 1.0000\n",
      "Epoch 121 train_loss: 1.1705 train_acc: 1.0000\n",
      "Epoch 122 train_loss: 1.1714 train_acc: 1.0000\n",
      "Epoch 123 train_loss: 1.1690 train_acc: 1.0000\n",
      "Epoch 124 train_loss: 1.1691 train_acc: 1.0000\n",
      "Epoch 125 train_loss: 1.1764 train_acc: 0.9929\n",
      "Epoch 126 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 127 train_loss: 1.1734 train_acc: 0.9929\n",
      "Epoch 128 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 129 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 130 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 131 train_loss: 1.1696 train_acc: 1.0000\n",
      "Epoch 132 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 133 train_loss: 1.1705 train_acc: 1.0000\n",
      "Epoch 134 train_loss: 1.1687 train_acc: 1.0000\n",
      "Epoch 135 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 136 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 137 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 138 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 139 train_loss: 1.1688 train_acc: 1.0000\n",
      "Epoch 140 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 141 train_loss: 1.1702 train_acc: 1.0000\n",
      "Epoch 142 train_loss: 1.1784 train_acc: 1.0000\n",
      "Epoch 143 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 144 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 145 train_loss: 1.1692 train_acc: 1.0000\n",
      "Epoch 146 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 147 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 148 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 149 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 150 train_loss: 1.1689 train_acc: 1.0000\n",
      "Epoch 151 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 152 train_loss: 1.1712 train_acc: 1.0000\n",
      "Epoch 153 train_loss: 1.1695 train_acc: 1.0000\n",
      "Epoch 154 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 155 train_loss: 1.1686 train_acc: 1.0000\n",
      "Epoch 156 train_loss: 1.1723 train_acc: 0.9929\n",
      "Epoch 157 train_loss: 1.1695 train_acc: 1.0000\n",
      "Epoch 158 train_loss: 1.1699 train_acc: 1.0000\n",
      "Epoch 159 train_loss: 1.1728 train_acc: 1.0000\n",
      "Epoch 160 train_loss: 1.1696 train_acc: 1.0000\n",
      "Epoch 161 train_loss: 1.1710 train_acc: 1.0000\n",
      "Epoch 162 train_loss: 1.1756 train_acc: 0.9929\n",
      "Epoch 163 train_loss: 1.1692 train_acc: 1.0000\n",
      "Epoch 164 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 165 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 166 train_loss: 1.1692 train_acc: 1.0000\n",
      "Epoch 167 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 168 train_loss: 1.1690 train_acc: 1.0000\n",
      "Epoch 169 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 170 train_loss: 1.1697 train_acc: 1.0000\n",
      "Epoch 171 train_loss: 1.1691 train_acc: 1.0000\n",
      "Epoch 172 train_loss: 1.1686 train_acc: 1.0000\n",
      "Epoch 173 train_loss: 1.1697 train_acc: 1.0000\n",
      "Epoch 174 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 175 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 176 train_loss: 1.1686 train_acc: 1.0000\n",
      "Epoch 177 train_loss: 1.1764 train_acc: 0.9929\n",
      "Epoch 178 train_loss: 1.1693 train_acc: 1.0000\n",
      "Epoch 179 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 180 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 181 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 182 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 183 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 184 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 185 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 186 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 187 train_loss: 1.1696 train_acc: 1.0000\n",
      "Epoch 188 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 189 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 190 train_loss: 1.1683 train_acc: 1.0000\n",
      "Epoch 191 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 192 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 193 train_loss: 1.1695 train_acc: 1.0000\n",
      "Epoch 194 train_loss: 1.1701 train_acc: 1.0000\n",
      "Epoch 195 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 196 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 197 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 198 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 199 train_loss: 1.1727 train_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    scheduler.step(loss)\n",
    "    optimizer.step()\n",
    "\n",
    "    _, pred = torch.max(out[data.train_mask], dim=1)\n",
    "    correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "    acc = correct/data.train_mask.sum().item()\n",
    "\n",
    "    print('Epoch {:03d} train_loss: {:.4f} train_acc: {:.4f}'.format(\n",
    "        epoch, loss.item(), acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
