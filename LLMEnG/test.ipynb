{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "dataset_cora = Planetoid(root='./cora/', name='Cora')\n",
    "# dataset = Planetoid(root='./citeseer',name='Citeseer')\n",
    "# dataset = Planetoid(root='./pubmed/',name='Pubmed')\n",
    "print(dataset_cora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btr/miniconda3/envs/LLMEnG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.1851, -0.2059, -1.8382,  ..., -2.8908,  1.3605,  0.3109],\n",
      "         [-1.8622, -0.9529, -2.4024,  ...,  1.0467,  1.3186,  1.0996],\n",
      "         [-0.2761, -0.4898, -0.9106,  ...,  1.2816,  0.6039,  1.7683],\n",
      "         ...,\n",
      "         [-0.3205, -2.7684, -2.1332,  ...,  1.1992,  0.9619,  1.0644],\n",
      "         [ 2.1264, -4.0235, -3.8833,  ..., -1.4924,  1.3204,  1.5962],\n",
      "         [-2.7754, -2.8158, -1.3051,  ..., -1.8278, -0.0244,  1.6202]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 定义模型和tokenizer的本地路径\n",
    "local_model_path = \"/home/btr/bpmn/model/safetensors/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# 加载预训练的tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModel.from_pretrained(local_model_path)\n",
    "\n",
    "# 要处理的输入文本\n",
    "text = \"这是一个示例文本\"\n",
    "\n",
    "# 将文本转换为tokenizer的输入格式\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型的最后一层隐藏状态作为嵌入表示\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 获取最后一层隐藏状态\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# 输出嵌入表示\n",
    "print(last_hidden_states)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 4096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset_cora.num_node_features, 16, 'lstm')\n",
    "        self.conv2 = SAGEConv(16, dataset_cora.num_classes, 'lstm')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(dataset_cora.num_node_features, 16, heads=2)\n",
    "        self.conv2 = GATConv(2*16, dataset_cora.num_classes, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 16, heads=2)\n",
      "  (conv2): GATConv(32, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GAT()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "data = dataset_cora[0].to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss: 1.9451 train_acc: 0.1857\n",
      "Epoch 001 train_loss: 1.9166 train_acc: 0.7214\n",
      "Epoch 002 train_loss: 1.8723 train_acc: 0.8071\n",
      "Epoch 003 train_loss: 1.8173 train_acc: 0.7786\n",
      "Epoch 004 train_loss: 1.7426 train_acc: 0.8000\n",
      "Epoch 005 train_loss: 1.6530 train_acc: 0.8929\n",
      "Epoch 006 train_loss: 1.5969 train_acc: 0.8714\n",
      "Epoch 007 train_loss: 1.5263 train_acc: 0.8571\n",
      "Epoch 008 train_loss: 1.4645 train_acc: 0.9143\n",
      "Epoch 009 train_loss: 1.4076 train_acc: 0.9500\n",
      "Epoch 010 train_loss: 1.3652 train_acc: 0.9429\n",
      "Epoch 011 train_loss: 1.3142 train_acc: 0.9714\n",
      "Epoch 012 train_loss: 1.2917 train_acc: 0.9571\n",
      "Epoch 013 train_loss: 1.2634 train_acc: 0.9714\n",
      "Epoch 014 train_loss: 1.2558 train_acc: 0.9714\n",
      "Epoch 015 train_loss: 1.2349 train_acc: 0.9857\n",
      "Epoch 016 train_loss: 1.2239 train_acc: 0.9786\n",
      "Epoch 017 train_loss: 1.2058 train_acc: 0.9857\n",
      "Epoch 018 train_loss: 1.2125 train_acc: 0.9714\n",
      "Epoch 019 train_loss: 1.2058 train_acc: 0.9786\n",
      "Epoch 020 train_loss: 1.1977 train_acc: 0.9786\n",
      "Epoch 021 train_loss: 1.1884 train_acc: 0.9929\n",
      "Epoch 022 train_loss: 1.1893 train_acc: 0.9857\n",
      "Epoch 023 train_loss: 1.1907 train_acc: 0.9857\n",
      "Epoch 024 train_loss: 1.1900 train_acc: 1.0000\n",
      "Epoch 025 train_loss: 1.1811 train_acc: 1.0000\n",
      "Epoch 026 train_loss: 1.1859 train_acc: 0.9857\n",
      "Epoch 027 train_loss: 1.1821 train_acc: 1.0000\n",
      "Epoch 028 train_loss: 1.1839 train_acc: 0.9929\n",
      "Epoch 029 train_loss: 1.1748 train_acc: 1.0000\n",
      "Epoch 030 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 031 train_loss: 1.1713 train_acc: 1.0000\n",
      "Epoch 032 train_loss: 1.1728 train_acc: 1.0000\n",
      "Epoch 033 train_loss: 1.1739 train_acc: 1.0000\n",
      "Epoch 034 train_loss: 1.1732 train_acc: 1.0000\n",
      "Epoch 035 train_loss: 1.1853 train_acc: 0.9929\n",
      "Epoch 036 train_loss: 1.1770 train_acc: 1.0000\n",
      "Epoch 037 train_loss: 1.1768 train_acc: 1.0000\n",
      "Epoch 038 train_loss: 1.1710 train_acc: 1.0000\n",
      "Epoch 039 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 040 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 041 train_loss: 1.1820 train_acc: 0.9929\n",
      "Epoch 042 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 043 train_loss: 1.1758 train_acc: 1.0000\n",
      "Epoch 044 train_loss: 1.1800 train_acc: 1.0000\n",
      "Epoch 045 train_loss: 1.1788 train_acc: 0.9929\n",
      "Epoch 046 train_loss: 1.1782 train_acc: 1.0000\n",
      "Epoch 047 train_loss: 1.1720 train_acc: 1.0000\n",
      "Epoch 048 train_loss: 1.1803 train_acc: 0.9929\n",
      "Epoch 049 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 050 train_loss: 1.1826 train_acc: 0.9929\n",
      "Epoch 051 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 052 train_loss: 1.1785 train_acc: 1.0000\n",
      "Epoch 053 train_loss: 1.1785 train_acc: 1.0000\n",
      "Epoch 054 train_loss: 1.1816 train_acc: 0.9929\n",
      "Epoch 055 train_loss: 1.1840 train_acc: 0.9929\n",
      "Epoch 056 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 057 train_loss: 1.1791 train_acc: 1.0000\n",
      "Epoch 058 train_loss: 1.1757 train_acc: 1.0000\n",
      "Epoch 059 train_loss: 1.1884 train_acc: 0.9857\n",
      "Epoch 060 train_loss: 1.1815 train_acc: 0.9929\n",
      "Epoch 061 train_loss: 1.1800 train_acc: 1.0000\n",
      "Epoch 062 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 063 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 064 train_loss: 1.1719 train_acc: 1.0000\n",
      "Epoch 065 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 066 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 067 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 068 train_loss: 1.1771 train_acc: 1.0000\n",
      "Epoch 069 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 070 train_loss: 1.1792 train_acc: 1.0000\n",
      "Epoch 071 train_loss: 1.1777 train_acc: 1.0000\n",
      "Epoch 072 train_loss: 1.1786 train_acc: 0.9929\n",
      "Epoch 073 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 074 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 075 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 076 train_loss: 1.1770 train_acc: 0.9929\n",
      "Epoch 077 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 078 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 079 train_loss: 1.1742 train_acc: 1.0000\n",
      "Epoch 080 train_loss: 1.1764 train_acc: 1.0000\n",
      "Epoch 081 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 082 train_loss: 1.1752 train_acc: 1.0000\n",
      "Epoch 083 train_loss: 1.1719 train_acc: 1.0000\n",
      "Epoch 084 train_loss: 1.1732 train_acc: 1.0000\n",
      "Epoch 085 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 086 train_loss: 1.1739 train_acc: 1.0000\n",
      "Epoch 087 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 088 train_loss: 1.1749 train_acc: 1.0000\n",
      "Epoch 089 train_loss: 1.1788 train_acc: 1.0000\n",
      "Epoch 090 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 091 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 092 train_loss: 1.1762 train_acc: 1.0000\n",
      "Epoch 093 train_loss: 1.1744 train_acc: 1.0000\n",
      "Epoch 094 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 095 train_loss: 1.1758 train_acc: 1.0000\n",
      "Epoch 096 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 097 train_loss: 1.1761 train_acc: 1.0000\n",
      "Epoch 098 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 099 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 100 train_loss: 1.1713 train_acc: 1.0000\n",
      "Epoch 101 train_loss: 1.1695 train_acc: 1.0000\n",
      "Epoch 102 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 103 train_loss: 1.1772 train_acc: 0.9929\n",
      "Epoch 104 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 105 train_loss: 1.1726 train_acc: 1.0000\n",
      "Epoch 106 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 107 train_loss: 1.1711 train_acc: 1.0000\n",
      "Epoch 108 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 109 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 110 train_loss: 1.1770 train_acc: 1.0000\n",
      "Epoch 111 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 112 train_loss: 1.1726 train_acc: 1.0000\n",
      "Epoch 113 train_loss: 1.1834 train_acc: 0.9857\n",
      "Epoch 114 train_loss: 1.1763 train_acc: 0.9929\n",
      "Epoch 115 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 116 train_loss: 1.1726 train_acc: 1.0000\n",
      "Epoch 117 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 118 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 119 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 120 train_loss: 1.1768 train_acc: 0.9929\n",
      "Epoch 121 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 122 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 123 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 124 train_loss: 1.1767 train_acc: 1.0000\n",
      "Epoch 125 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 126 train_loss: 1.1763 train_acc: 0.9929\n",
      "Epoch 127 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 128 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 129 train_loss: 1.1700 train_acc: 1.0000\n",
      "Epoch 130 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 131 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 132 train_loss: 1.1709 train_acc: 1.0000\n",
      "Epoch 133 train_loss: 1.1737 train_acc: 1.0000\n",
      "Epoch 134 train_loss: 1.1750 train_acc: 1.0000\n",
      "Epoch 135 train_loss: 1.1704 train_acc: 1.0000\n",
      "Epoch 136 train_loss: 1.1737 train_acc: 1.0000\n",
      "Epoch 137 train_loss: 1.1719 train_acc: 1.0000\n",
      "Epoch 138 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 139 train_loss: 1.1761 train_acc: 0.9929\n",
      "Epoch 140 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 141 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 142 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 143 train_loss: 1.1729 train_acc: 1.0000\n",
      "Epoch 144 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 145 train_loss: 1.1756 train_acc: 1.0000\n",
      "Epoch 146 train_loss: 1.1736 train_acc: 1.0000\n",
      "Epoch 147 train_loss: 1.1747 train_acc: 1.0000\n",
      "Epoch 148 train_loss: 1.1740 train_acc: 1.0000\n",
      "Epoch 149 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 150 train_loss: 1.1757 train_acc: 1.0000\n",
      "Epoch 151 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 152 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 153 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 154 train_loss: 1.1779 train_acc: 0.9929\n",
      "Epoch 155 train_loss: 1.1721 train_acc: 1.0000\n",
      "Epoch 156 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 157 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 158 train_loss: 1.1733 train_acc: 1.0000\n",
      "Epoch 159 train_loss: 1.1735 train_acc: 1.0000\n",
      "Epoch 160 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 161 train_loss: 1.1751 train_acc: 0.9929\n",
      "Epoch 162 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 163 train_loss: 1.1706 train_acc: 1.0000\n",
      "Epoch 164 train_loss: 1.1745 train_acc: 1.0000\n",
      "Epoch 165 train_loss: 1.1778 train_acc: 0.9929\n",
      "Epoch 166 train_loss: 1.1722 train_acc: 1.0000\n",
      "Epoch 167 train_loss: 1.1759 train_acc: 1.0000\n",
      "Epoch 168 train_loss: 1.1724 train_acc: 1.0000\n",
      "Epoch 169 train_loss: 1.1698 train_acc: 1.0000\n",
      "Epoch 170 train_loss: 1.1727 train_acc: 1.0000\n",
      "Epoch 171 train_loss: 1.1780 train_acc: 1.0000\n",
      "Epoch 172 train_loss: 1.1750 train_acc: 1.0000\n",
      "Epoch 173 train_loss: 1.1799 train_acc: 0.9929\n",
      "Epoch 174 train_loss: 1.1741 train_acc: 1.0000\n",
      "Epoch 175 train_loss: 1.1739 train_acc: 1.0000\n",
      "Epoch 176 train_loss: 1.1768 train_acc: 0.9929\n",
      "Epoch 177 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 178 train_loss: 1.1708 train_acc: 1.0000\n",
      "Epoch 179 train_loss: 1.1716 train_acc: 1.0000\n",
      "Epoch 180 train_loss: 1.1761 train_acc: 0.9929\n",
      "Epoch 181 train_loss: 1.1712 train_acc: 1.0000\n",
      "Epoch 182 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 183 train_loss: 1.1707 train_acc: 1.0000\n",
      "Epoch 184 train_loss: 1.1703 train_acc: 1.0000\n",
      "Epoch 185 train_loss: 1.1801 train_acc: 0.9929\n",
      "Epoch 186 train_loss: 1.1738 train_acc: 1.0000\n",
      "Epoch 187 train_loss: 1.1747 train_acc: 0.9929\n",
      "Epoch 188 train_loss: 1.1715 train_acc: 1.0000\n",
      "Epoch 189 train_loss: 1.1717 train_acc: 1.0000\n",
      "Epoch 190 train_loss: 1.1730 train_acc: 1.0000\n",
      "Epoch 191 train_loss: 1.1743 train_acc: 1.0000\n",
      "Epoch 192 train_loss: 1.1718 train_acc: 1.0000\n",
      "Epoch 193 train_loss: 1.1767 train_acc: 0.9929\n",
      "Epoch 194 train_loss: 1.1723 train_acc: 1.0000\n",
      "Epoch 195 train_loss: 1.1725 train_acc: 1.0000\n",
      "Epoch 196 train_loss: 1.1714 train_acc: 1.0000\n",
      "Epoch 197 train_loss: 1.1734 train_acc: 1.0000\n",
      "Epoch 198 train_loss: 1.1694 train_acc: 1.0000\n",
      "Epoch 199 train_loss: 1.1711 train_acc: 1.0000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, pred = torch.max(out[data.train_mask], dim=1)\n",
    "    correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "    acc = correct/data.train_mask.sum().item()\n",
    "\n",
    "    print('Epoch {:03d} train_loss: {:.4f} train_acc: {:.4f}'.format(\n",
    "        epoch, loss.item(), acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
